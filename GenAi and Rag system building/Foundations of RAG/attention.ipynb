{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122a7389",
   "metadata": {},
   "source": [
    "### **Attention in RAG (Retrieval-Augmented Generation)**  \n",
    "\n",
    "#### **Definition**  \n",
    "**Attention** is a mechanism that allows models to **focus on the most relevant parts** of the input text when generating an answer. In RAG, attention helps:  \n",
    "- Decide **which retrieved documents** are most useful.  \n",
    "- Determine **which parts of those documents** to emphasize when generating a response.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Do We Need Attention in RAG?**  \n",
    "1. **Handles Long Contexts**  \n",
    "   - RAG retrieves multiple documents → attention helps pick key sections instead of processing everything.  \n",
    "2. **Improves Answer Quality**  \n",
    "   - Weights important words (e.g., \"Einstein\" in a query about relativity).  \n",
    "3. **Reduces Noise**  \n",
    "   - Ignores irrelevant parts of retrieved text.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Attention Works (Simple Example)**  \n",
    "**Scenario:**  \n",
    "- **Query:** *\"Who invented the theory of relativity?\"*  \n",
    "- **Retrieved Document:** *\"Albert Einstein, a physicist, developed the theory of relativity in 1905.\"*  \n",
    "\n",
    "**Without Attention:**  \n",
    "The model might equally process all words, including less relevant ones like \"physicist\" or \"1905.\"  \n",
    "\n",
    "**With Attention:**  \n",
    "The model **focuses more** on:  \n",
    "- **\"Albert Einstein\"** (who)  \n",
    "- **\"theory of relativity\"** (what)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Attention in RAG**  \n",
    "1. **Cross-Attention (Query-to-Document Attention)**  \n",
    "   - The query \"attends\" to relevant parts of retrieved documents.  \n",
    "2. **Self-Attention (Within-Document Attention)**  \n",
    "   - The model understands relationships between words in a single document (e.g., \"Einstein\" → \"physicist\").  \n",
    "\n",
    "---\n",
    "\n",
    "### **Attention in Code (Simplified Example)**  \n",
    "Here’s how attention is implemented in a Hugging Face RAG model:  \n",
    "\n",
    "```python\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "import torch\n",
    "\n",
    "# Load RAG model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n",
    "\n",
    "# Query and retrieved documents\n",
    "query = \"Who invented the theory of relativity?\"\n",
    "docs = [\"Albert Einstein, a physicist, developed the theory of relativity in 1905.\"]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "doc_inputs = tokenizer(docs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Forward pass (attention happens inside the model)\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    doc_scores=torch.tensor([[1.0]]),  # Simulate retrieval score\n",
    "    doc_ids=torch.tensor([[0]]),        # Simulate top-1 document\n",
    "    decoder_input_ids=inputs[\"input_ids\"],\n",
    ")\n",
    "\n",
    "# Generate answer (uses attention to focus on key parts)\n",
    "generated = model.generate(input_ids=inputs[\"input_ids\"])\n",
    "answer = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(f\"Answer: {answer}\")\n",
    "```\n",
    "\n",
    "**Output:**  \n",
    "```\n",
    "Answer: Albert Einstein invented the theory of relativity.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **How Attention is Applied in This Code**  \n",
    "1. **Retrieval Phase:**  \n",
    "   - The retriever fetches relevant documents (here, hardcoded for simplicity).  \n",
    "2. **Cross-Attention:**  \n",
    "   - The model compares the query (`\"Who invented...\"`) with the document (`\"Albert Einstein...\"`).  \n",
    "3. **Generation Phase:**  \n",
    "   - The decoder uses attention to focus on `\"Albert Einstein\"` and `\"theory of relativity\"` while generating the answer.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**  \n",
    "- Attention helps RAG **filter noise** and **focus on key information**.  \n",
    "- It’s **automatically handled** in transformer models (like BERT, T5).  \n",
    "- Without attention, RAG would struggle with long or noisy retrieved documents.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec4dab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
