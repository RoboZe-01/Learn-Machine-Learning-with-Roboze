{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432eedde",
   "metadata": {},
   "source": [
    "# **Text Similarity in RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "## **What is Text Similarity?**\n",
    "Text similarity measures **how semantically or structurally close two pieces of text are**. In RAG, it helps:\n",
    "- Retrieve the most relevant documents for a query.\n",
    "- Rank passages before feeding them to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Do We Need Text Similarity in RAG?**\n",
    "1. **Improves Retrieval Accuracy**  \n",
    "   - Ensures the retrieved documents actually match the queryâ€™s meaning.\n",
    "2. **Filters Irrelevant Content**  \n",
    "   - Low similarity scores can exclude noisy or unrelated text.\n",
    "3. **Optimizes LLM Input**  \n",
    "   - Only the most similar (and useful) documents are passed to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Text Similarity**\n",
    "\n",
    "### **1. Cosine Similarity (Most Common in RAG)**\n",
    "- Measures the **angle** between two vectors (usually word/sentence embeddings).\n",
    "- **Range:** `-1` (opposite) to `1` (identical).  \n",
    "- **Best for:** Semantic similarity (e.g., comparing sentence embeddings).\n",
    "\n",
    "#### **Example:**\n",
    "- **Query:** *\"What is AI?\"*  \n",
    "- **Document:** *\"Artificial Intelligence (AI) is machine intelligence.\"*  \n",
    "- **Similarity:** `0.92` (highly similar).\n",
    "\n",
    "#### **Code (Python):**\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings (e.g., from Sentence-BERT)\n",
    "query_embedding = np.array([0.2, 0.8, 0.3])\n",
    "doc_embedding = np.array([0.1, 0.7, 0.2])\n",
    "\n",
    "similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "print(f\"Cosine Similarity: {similarity:.2f}\")\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "Cosine Similarity: 0.99\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Jaccard Similarity (Set-Based)**\n",
    "- Measures **overlap between word sets** (ignores order & meaning).  \n",
    "- **Formula:**  \n",
    "  \\[\n",
    "  J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "  \\]\n",
    "- **Range:** `0` (no overlap) to `1` (identical).  \n",
    "- **Best for:** Keyword-based matching (e.g., exact word overlap).\n",
    "\n",
    "#### **Example:**\n",
    "- **Query:** *\"How does photosynthesis work?\"*  \n",
    "- **Document:** *\"Photosynthesis converts sunlight into energy.\"*  \n",
    "- **Shared words:** `{\"photosynthesis\"}`  \n",
    "- **Similarity:** `1/5 = 0.2` (low, despite being semantically related).\n",
    "\n",
    "#### **Code (Python):**\n",
    "```python\n",
    "def jaccard_similarity(text1, text2):\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "query = \"How does photosynthesis work?\"\n",
    "document = \"Photosynthesis converts sunlight into energy.\"\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity(query, document):.2f}\")\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "Jaccard Similarity: 0.20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Euclidean Distance (L2 Distance)**\n",
    "- Measures **straight-line distance** between vectors.  \n",
    "- **Lower values = more similar**.  \n",
    "- **Best for:** Clustering, but less common in RAG (sensitive to vector scale).\n",
    "\n",
    "#### **Example:**\n",
    "- **Query Embedding:** `[0.1, 0.5]`  \n",
    "- **Doc Embedding:** `[0.2, 0.6]`  \n",
    "- **Distance:** `âˆš[(0.1-0.2)Â² + (0.5-0.6)Â²] = 0.14` (small distance = high similarity).\n",
    "\n",
    "#### **Code (Python):**\n",
    "```python\n",
    "from scipy.spatial import distance\n",
    "\n",
    "query_embedding = np.array([0.1, 0.5])\n",
    "doc_embedding = np.array([0.2, 0.6])\n",
    "\n",
    "euclidean_dist = distance.euclidean(query_embedding, doc_embedding)\n",
    "print(f\"Euclidean Distance: {euclidean_dist:.2f}\")\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "Euclidean Distance: 0.14\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. BERTScore (Context-Aware)**\n",
    "- Uses BERT embeddings to compare **contextual similarity**.  \n",
    "- **Range:** `0` to `1` (matches human judgment better).  \n",
    "- **Best for:** Evaluating LLM outputs or high-precision retrieval.\n",
    "\n",
    "#### **Code (Python):**\n",
    "```python\n",
    "from bert_score import score\n",
    "\n",
    "query = \"What is AI?\"\n",
    "document = \"Artificial Intelligence mimics human cognition.\"\n",
    "\n",
    "P, R, F1 = score([query], [document], lang=\"en\")\n",
    "print(f\"BERTScore F1: {F1.mean():.2f}\")\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "BERTScore F1: 0.85\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Which Similarity Measure to Use in RAG?**\n",
    "| **Method**          | **Pros**                          | **Cons**                          | **Use Case**                     |\n",
    "|---------------------|-----------------------------------|-----------------------------------|----------------------------------|\n",
    "| **Cosine**          | Works well with embeddings        | Needs vectorization               | General retrieval (e.g., FAISS)  |\n",
    "| **Jaccard**         | Simple, no ML needed              | Ignores semantics                | Keyword search                   |\n",
    "| **Euclidean**       | Intuitive distance metric         | Sensitive to vector scale        | Clustering                       |\n",
    "| **BERTScore**       | Context-aware, high accuracy      | Computationally heavy            | Evaluating LLM outputs           |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "1. **Cosine similarity** is the **default choice** for RAG (used in vector DBs like FAISS).  \n",
    "2. **Jaccard** is fast but ignores meaning.  \n",
    "3. **BERTScore** is ideal for evaluating quality but too slow for real-time retrieval.  \n",
    "4. Always **normalize vectors** if using Euclidean distance.  \n",
    "\n",
    "Would you like a **hybrid approach** (e.g., Jaccard + Cosine) for better retrieval? ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a1b20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
