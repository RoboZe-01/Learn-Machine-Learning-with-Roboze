{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5e047",
   "metadata": {},
   "source": [
    "### **Transformers in RAG (Retrieval-Augmented Generation)**  \n",
    "\n",
    "#### **Definition**  \n",
    "Transformers are **deep learning models** that process entire sequences of text (like sentences or documents) **all at once** instead of word-by-word (like older RNNs). In RAG, they enable:  \n",
    "- **Full-context understanding** → The model sees relationships between all words in a query + retrieved documents.  \n",
    "- **Parallel processing** → Faster than sequential models (e.g., LSTMs).  \n",
    "- **Self-attention** → Dynamically focuses on the most relevant words.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Transformers Are Crucial for RAG**  \n",
    "1. **Handles Long Texts**  \n",
    "   - Traditional models (like RNNs) struggle with long documents. Transformers process **all retrieved text in one go**.  \n",
    "2. **Captures Complex Relationships**  \n",
    "   - Understands connections like *\"Einstein → physicist → relativity\"* across sentences.  \n",
    "3. **Efficient Retrieval-Augmentation**  \n",
    "   - The transformer’s **cross-attention** links the query to the most relevant parts of retrieved docs.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Transformers Work in RAG (Simple Example)**  \n",
    "**Scenario:**  \n",
    "- **Query:** *\"What is the capital of France?\"*  \n",
    "- **Retrieved Document:** *\"France is a country in Europe. Its capital is Paris.\"*  \n",
    "\n",
    "**Transformer’s Role:**  \n",
    "1. **Encodes the Query & Document**  \n",
    "   - Converts text into numerical vectors (embeddings).  \n",
    "2. **Applies Self-Attention**  \n",
    "   - For the query, it links *\"capital\"* to *\"France\"*.  \n",
    "   - For the document, it links *\"France\"* to *\"Paris\"*.  \n",
    "3. **Cross-Attention (Query → Document)**  \n",
    "   - The query’s *\"capital\"* focuses on the document’s *\"Paris\"*.  \n",
    "4. **Generates Answer**  \n",
    "   - Outputs *\"The capital of France is Paris.\"*  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components of Transformers in RAG**  \n",
    "| Component          | Role in RAG                                                                 |\n",
    "|--------------------|-----------------------------------------------------------------------------|\n",
    "| **Encoder**        | Processes retrieved documents (e.g., BERT).                                 |\n",
    "| **Decoder**        | Generates answers (e.g., T5). In RAG, often a single model handles both.    |\n",
    "| **Self-Attention** | Finds relationships within a single text (e.g., links \"France\" → \"Paris\").  |\n",
    "| **Cross-Attention**| Connects the query to document parts (e.g., query \"capital\" → doc \"Paris\"). |\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformers in RAG: Code Example**  \n",
    "Here’s how a transformer-based RAG model processes a query:  \n",
    "\n",
    "```python\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "# Load pre-trained RAG model (uses transformers internally)\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "# Query and documents\n",
    "query = \"What is the capital of France?\"\n",
    "documents = [\"France is a country in Europe. Its capital is Paris.\"]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "doc_inputs = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Forward pass (transformer processes query + documents)\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    doc_scores=torch.tensor([[1.0]]),  # Simulate retrieval score\n",
    "    doc_ids=torch.tensor([[0]]),        # Simulate top-1 document\n",
    ")\n",
    "\n",
    "# Generate answer\n",
    "answer = tokenizer.decode(outputs[\"generated_text\"][0], skip_special_tokens=True)\n",
    "print(f\"Answer: {answer}\")\n",
    "```\n",
    "\n",
    "**Output:**  \n",
    "```\n",
    "Answer: The capital of France is Paris.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Happens Under the Hood?**  \n",
    "1. **Tokenization**  \n",
    "   - The query/document is split into tokens (e.g., `[\"What\", \"is\", \"the\", \"capital\", ...]`).  \n",
    "2. **Embedding Layer**  \n",
    "   - Each token is converted to a vector.  \n",
    "3. **Transformer Layers**  \n",
    "   - **Self-attention:** The document’s tokens interact (e.g., *\"France\"* attends to *\"Paris\"*).  \n",
    "   - **Cross-attention:** The query’s *\"capital\"* attends to the document’s *\"Paris\"*.  \n",
    "4. **Generation**  \n",
    "   - The decoder predicts the answer token-by-token using attention.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Transformers > Older Models in RAG**  \n",
    "| Feature               | Transformers | RNNs/LSTMs       |\n",
    "|-----------------------|-------------|------------------|\n",
    "| **Context Handling**  | Full-sequence at once | Word-by-word (loses context) |\n",
    "| **Parallel Processing**| Yes (faster) | No (sequential)  |\n",
    "| **Attention**         | Self + Cross-attention | Limited          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**  \n",
    "- Transformers allow RAG to **understand and connect** queries + retrieved documents **in one pass**.  \n",
    "- **Self-attention** finds relationships within a text.  \n",
    "- **Cross-attention** links the query to the right parts of documents.  \n",
    "- Without transformers, RAG would be slower and less accurate.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1c104",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
