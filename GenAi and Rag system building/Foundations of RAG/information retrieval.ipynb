{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50759841",
   "metadata": {},
   "source": [
    "# **Information Retrieval (IR) in AI: A Step-by-Step Guide**\n",
    "\n",
    "## **What is Information Retrieval?**\n",
    "Information Retrieval (IR) is the process of **finding relevant information** from large datasets (documents, web pages, databases) in response to a user query. It powers:\n",
    "- Search engines (Google, Bing)\n",
    "- RAG systems\n",
    "- Document search tools\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is IR Important?**\n",
    "1. **Efficiency** → Quickly finds needles in haystacks.\n",
    "2. **Precision** → Returns the most relevant results.\n",
    "3. **Scalability** → Works on terabytes of data.\n",
    "\n",
    "---\n",
    "\n",
    "# **Step-by-Step Information Retrieval Process**\n",
    "\n",
    "### **1. Document Collection**\n",
    "- **Goal:** Gather raw data (web pages, PDFs, databases).\n",
    "- **Sources:**  \n",
    "  - Web crawlers (for search engines)  \n",
    "  - Internal databases (for enterprise search)  \n",
    "  - APIs (e.g., PubMed for medical papers)  \n",
    "\n",
    "#### **Example:**\n",
    "A search engine crawls Wikipedia to build its index.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Preprocessing**\n",
    "Raw text is cleaned and standardized:\n",
    "- **Tokenization**: Split text into words/tokens (`\"Hello world!\" → [\"Hello\", \"world\"]`).  \n",
    "- **Stopword Removal**: Discard common words (`\"the\", \"and\"`).  \n",
    "- **Stemming/Lemmatization**: Reduce words to root forms (`\"running\" → \"run\"`).  \n",
    "- **Lowercasing**: Ensure case insensitivity (`\"Apple\" → \"apple\"`).  \n",
    "\n",
    "#### **Code (Python):**\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "tokens = word_tokenize(text.lower())  # Lowercase + tokenize\n",
    "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  # Stemming\n",
    "\n",
    "print(stemmed_tokens)\n",
    "```\n",
    "**Output:**  \n",
    "`['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']`\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Indexing (Building Searchable Structures)**\n",
    "Convert documents into a search-optimized format:\n",
    "- **Inverted Index**: Maps words → document IDs (like a book index).  \n",
    "  ```\n",
    "  \"apple\" → [Doc1, Doc3, Doc5]\n",
    "  \"banana\" → [Doc2, Doc4]\n",
    "  ```\n",
    "- **Vector Index**: Stores embeddings for semantic search (used in RAG).\n",
    "\n",
    "#### **Tools:**\n",
    "- **Elasticsearch** (for keyword search)  \n",
    "- **FAISS** (for vector similarity search)  \n",
    "\n",
    "#### **Code (Inverted Index in Python):**\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "documents = {\n",
    "    \"Doc1\": \"apple banana\",\n",
    "    \"Doc2\": \"banana orange\",\n",
    "    \"Doc3\": \"apple orange\",\n",
    "}\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for doc_id, text in documents.items():\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        inverted_index[token].append(doc_id)\n",
    "\n",
    "print(dict(inverted_index))\n",
    "```\n",
    "**Output:**  \n",
    "`{'apple': ['Doc1', 'Doc3'], 'banana': ['Doc1', 'Doc2'], 'orange': ['Doc2', 'Doc3']}`\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Query Processing**\n",
    "- **Tokenize/Normalize** the user query (same as preprocessing).  \n",
    "- **Expand Query** (Optional):  \n",
    "  - Add synonyms (`\"car\" → [\"auto\", \"vehicle\"]`).  \n",
    "  - Use spell check (`\"googel\" → \"google\"`).  \n",
    "\n",
    "#### **Example:**\n",
    "Query: `\"best smartphones 2024\"`  \n",
    "Processed: `[\"best\", \"smartphon\", \"2024\"]` (after stemming + stopword removal)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Retrieval & Ranking**\n",
    "#### **A. Keyword Search (TF-IDF/BM25)**\n",
    "- **TF-IDF**: Weights words by frequency in doc vs. rarity in corpus.  \n",
    "- **BM25**: Improved TF-IDF (handles doc length better).  \n",
    "\n",
    "#### **B. Semantic Search (Embeddings)**\n",
    "- Encode query/documents into vectors.  \n",
    "- Use **cosine similarity** to rank by meaning.  \n",
    "\n",
    "#### **Code (BM25 with `rank_bm25`):**\n",
    "```python\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"apple banana fruit\",\n",
    "    \"banana orange fruit\",\n",
    "    \"apple orange juice\",\n",
    "]\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "query = \"apple fruit\"\n",
    "tokenized_query = query.split()\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "print(\"BM25 Scores:\", doc_scores)\n",
    "```\n",
    "**Output:**  \n",
    "`BM25 Scores: [1.38, 0.69, 0.41]`  \n",
    "*(Doc1 is most relevant)*\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Re-Ranking (Optional)**\n",
    "Improve results with:  \n",
    "- **Cross-Encoders** (BERT models that compare query/doc pairs in detail).  \n",
    "- **Learning-to-Rank (LTR)**: ML models trained to optimize ranking.  \n",
    "\n",
    "#### **Code (Cross-Encoder Re-Ranking):**\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "query = \"What is AI?\"\n",
    "docs = [\n",
    "    \"Artificial Intelligence (AI) simulates human thinking.\",\n",
    "    \"AI is used in chatbots like ChatGPT.\",\n",
    "]\n",
    "scores = cross_encoder.predict([(query, doc) for doc in docs])\n",
    "\n",
    "print(\"Cross-Encoder Scores:\", scores)\n",
    "```\n",
    "**Output:**  \n",
    "`Cross-Encoder Scores: [8.92, 5.31]`  \n",
    "*(First doc is better)*\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Evaluation Metrics**\n",
    "Measure IR system quality:  \n",
    "- **Precision@K**: % of top-K results that are relevant.  \n",
    "- **Recall@K**: % of all relevant docs found in top-K.  \n",
    "- **Mean Reciprocal Rank (MRR)**: Rank of first relevant result.  \n",
    "\n",
    "#### **Example:**\n",
    "- **Precision@3**: If 2 of top 3 results are relevant → `2/3 = 0.67`.  \n",
    "- **MRR**: First relevant doc is at position 2 → `1/2 = 0.5`.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key IR Algorithms**\n",
    "| **Algorithm** | **Type**       | **Best For**                     |\n",
    "|---------------|----------------|----------------------------------|\n",
    "| **TF-IDF**    | Keyword        | Simple term matching             |\n",
    "| **BM25**      | Keyword        | Better than TF-IDF (handles length) |\n",
    "| **Word2Vec**  | Semantic       | Word-level similarity            |\n",
    "| **BERT**      | Semantic       | Context-aware search             |\n",
    "\n",
    "---\n",
    "\n",
    "## **Advanced IR Techniques**\n",
    "1. **Query Expansion**: Add synonyms/spelling variants.  \n",
    "2. **Dense Retrieval**: Use transformers (e.g., DPR, ANCE).  \n",
    "3. **Hybrid Search**: Combine keyword + vector search.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Real-World IR Systems**\n",
    "1. **Google Search**: BM25 + BERT + PageRank.  \n",
    "2. **RAG**: FAISS (vector search) + BM25.  \n",
    "3. **Spotlight (Apple)**: Semantic + location-aware search.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Summary: IR Pipeline**\n",
    "1. **Collect** → Crawl data.  \n",
    "2. **Preprocess** → Clean/normalize text.  \n",
    "3. **Index** → Build inverted/vector indexes.  \n",
    "4. **Query** → Process user input.  \n",
    "5. **Retrieve** → Fetch candidate docs.  \n",
    "6. **Rank** → Sort by relevance (BM25/embeddings).  \n",
    "7. **Evaluate** → Measure performance.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
