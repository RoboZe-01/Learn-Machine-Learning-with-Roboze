{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8b8d58",
   "metadata": {},
   "source": [
    "### **Tokenization in RAG (Retrieval-Augmented Generation)**  \n",
    "\n",
    "#### **Definition**  \n",
    "Tokenization is the process of splitting text into smaller units (**tokens**) such as words, subwords, or characters. It is a crucial preprocessing step in NLP and RAG pipelines to convert raw text into a format that machine learning models can process.  \n",
    "\n",
    "#### **Why is Tokenization Needed?**  \n",
    "1. **Model Input Standardization** â†’ Neural networks require structured input (numbers, not raw text).  \n",
    "2. **Handling Variable-Length Text** â†’ Breaks long documents into manageable chunks.  \n",
    "3. **Improves Retrieval Accuracy** â†’ Helps match search queries with relevant passages.  \n",
    "4. **Supports Subword Understanding** â†’ Handles rare/unseen words by splitting them into meaningful parts.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Tokenization Works**  \n",
    "#### **1. Word Tokenization**  \n",
    "- Splits text into words using spaces/punctuation.  \n",
    "- Example: `\"Don't stop!\"` â†’ `[\"Don\", \"'\", \"t\", \"stop\", \"!\"]`  \n",
    "\n",
    "#### **2. Subword Tokenization (Used in Modern LLMs)**  \n",
    "- Splits words into smaller frequent units (e.g., `\"unhappiness\"` â†’ `\"un\", \"happiness\"`).  \n",
    "- Popular methods: **Byte-Pair Encoding (BPE), WordPiece, Unigram**.  \n",
    "\n",
    "#### **3. Character Tokenization**  \n",
    "- Splits text into individual characters (rarely used in RAG).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tokenization in Code (Python Examples)**  \n",
    "\n",
    "#### **1. Using `split()` (Naive Word Tokenization)**  \n",
    "```python\n",
    "text = \"Tokenization is essential for NLP.\"\n",
    "tokens = text.split()  # Splits by whitespace\n",
    "print(tokens)\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "['Tokenization', 'is', 'essential', 'for', 'NLP.']\n",
    "```\n",
    "**Problem:** Doesnâ€™t handle punctuation well (`NLP.` should ideally be `NLP` + `.`).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Using `nltk` (Better Word Tokenization)**  \n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download tokenizer data\n",
    "\n",
    "text = \"Don't stop! This is NLP.\"\n",
    "tokens = nltk.word_tokenize(text)  # Handles contractions & punctuation\n",
    "print(tokens)\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "['Do', \"n't\", 'stop', '!', 'This', 'is', 'NLP', '.']\n",
    "```\n",
    "**Improvement:** Splits contractions (`Don't` â†’ `Do` + `n't`) and punctuation.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Using Hugging Face Tokenizers (Subword Tokenization - BERT/GPT Style)**  \n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
    "text = \"Tokenization in RAG is powerful!\"\n",
    "tokens = tokenizer.tokenize(text)  # Uses WordPiece subword tokenization\n",
    "print(tokens)\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "['token', '##ization', 'in', 'rag', 'is', 'powerful', '!']\n",
    "```\n",
    "**Key Features:**  \n",
    "- Converts to lowercase (`RAG` â†’ `rag`).  \n",
    "- Splits complex words (`Tokenization` â†’ `token` + `##ization`).  \n",
    "- `##` indicates a subword continuation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Tokenization Helps in RAG**  \n",
    "1. **Efficient Retrieval** â†’ Ensures search queries and documents are split into comparable tokens.  \n",
    "2. **Handles OOV (Out-of-Vocabulary) Words** â†’ Subword tokenization can process rare/unseen words.  \n",
    "3. **Compatibility with LLMs** â†’ Models like BERT/GPT require tokenized input.  \n",
    "\n",
    "### **When to Use Which Tokenizer?**  \n",
    "| **Tokenizer Type** | **Best For** | **Example** |\n",
    "|--------------------|-------------|-------------|\n",
    "| **Word Tokenizer** | Simple NLP tasks | `nltk.word_tokenize()` |  \n",
    "| **Subword Tokenizer** | LLMs (BERT, GPT) | Hugging Face `AutoTokenizer` |  \n",
    "| **Character Tokenizer** | Rarely used in RAG | `list(\"text\")` |  \n",
    "\n",
    "Would you like a deeper dive into **BPE/WordPiece tokenization**? ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf5cad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
