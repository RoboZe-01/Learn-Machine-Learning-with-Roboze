{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e88ade6",
   "metadata": {},
   "source": [
    "### **Embeddings in RAG (Retrieval-Augmented Generation)**  \n",
    "\n",
    "#### **Definition**  \n",
    "Embeddings are **numerical representations** of text (words, sentences, or documents) in a high-dimensional vector space. They capture semantic meaning, allowing machines to understand relationships between words/phrases (e.g., *\"king\" – \"man\" + \"woman\" ≈ \"queen\"*).  \n",
    "\n",
    "In RAG:  \n",
    "- **Retrieval Phase:** Embeddings help find the most relevant documents for a query.  \n",
    "- **Generation Phase:** The LLM uses these embeddings to generate accurate answers.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Are Embeddings Needed?**  \n",
    "1. **Semantic Search**  \n",
    "   - Matches queries with documents **based on meaning** (not just keywords).  \n",
    "   - Example: *\"Canines\"* should match documents about *\"dogs\"*.  \n",
    "\n",
    "2. **Dense Vector Representation**  \n",
    "   - Converts text into compact numerical vectors (e.g., 384 or 768 dimensions).  \n",
    "   - Enables fast similarity calculations (e.g., cosine similarity).  \n",
    "\n",
    "3. **Handles Context**  \n",
    "   - Modern embeddings (e.g., BERT, OpenAI) understand polysemy (e.g., *\"bank\"* as river vs. financial).  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Embeddings Work**  \n",
    "1. **Input Text** → Tokenized into words/subwords.  \n",
    "2. **Embedding Model** → Maps tokens to vectors (using neural networks).  \n",
    "3. **Aggregation** → Combines token embeddings into a single vector (e.g., mean pooling).  \n",
    "\n",
    "#### **Types of Embeddings**  \n",
    "| **Type**          | **Description**                     | **Example Models**              |  \n",
    "|-------------------|-------------------------------------|----------------------------------|  \n",
    "| **Word Embeddings** | Static vectors per word.            | Word2Vec, GloVe                  |  \n",
    "| **Contextual Embeddings** | Varies based on sentence context. | BERT, RoBERTa, OpenAI Embeddings |  \n",
    "| **Sentence Embeddings** | Represents full sentences.         | Sentence-BERT, All-MiniLM-L6-v2  |  \n",
    "\n",
    "---\n",
    "\n",
    "### **Embeddings in Code (Python Examples)**  \n",
    "\n",
    "#### **1. Word Embeddings (GloVe)**  \n",
    "```python\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "\n",
    "# Get embedding for a word\n",
    "vector = glove_vectors['king']  # Shape: (100,)\n",
    "print(f\"Embedding for 'king': {vector[:5]}...\")  # Show first 5 dimensions\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "Embedding for 'king': [-0.23192   0.023731  0.31837  -0.14263  -0.087306]...\n",
    "```\n",
    "**Limitation:** No context (e.g., \"bank\" always has the same vector).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Contextual Embeddings (Sentence-BERT)**  \n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model\n",
    "\n",
    "# Encode sentences\n",
    "sentences = [\"Dogs are loyal.\", \"Canines are faithful.\"]\n",
    "embeddings = model.encode(sentences)  # Shape: (2, 384)\n",
    "\n",
    "# Compare similarity\n",
    "similarity = np.dot(embeddings[0], embeddings[1])  # Cosine similarity\n",
    "print(f\"Similarity: {similarity:.4f}\")\n",
    "```\n",
    "**Output:**  \n",
    "```\n",
    "Similarity: 0.8412  # High similarity due to semantic meaning\n",
    "```\n",
    "**Advantage:** Understands synonyms (*\"dogs\" ≈ \"canines\"*) and context.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. OpenAI Embeddings (for RAG)**  \n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "text = \"What is Retrieval-Augmented Generation?\"\n",
    "\n",
    "# Get embedding\n",
    "response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "embedding = response['data'][0]['embedding']  # Shape: (1536,)\n",
    "print(f\"Vector length: {len(embedding)}\")\n",
    "```\n",
    "**Use Case:** Powering vector databases (e.g., FAISS, Pinecone) in RAG systems.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How Embeddings Help in RAG**  \n",
    "1. **Retrieval Phase**  \n",
    "   - Queries/documents are converted to vectors.  \n",
    "   - A **vector database** finds the closest matches (e.g., using cosine similarity).  \n",
    "\n",
    "2. **Generation Phase**  \n",
    "   - The LLM uses retrieved documents (passed as context) to generate answers.  \n",
    "\n",
    "3. **Efficiency**  \n",
    "   - Dense embeddings (e.g., SBERT) are faster than sparse BM25 for large-scale search.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing an Embedding Model**  \n",
    "| **Model**               | **Dimensions** | **When to Use**                     |  \n",
    "|-------------------------|---------------|--------------------------------------|  \n",
    "| **Word2Vec/GloVe**      | 50-300        | Simple word-level tasks.             |  \n",
    "| **Sentence-BERT**       | 384-768       | Balance of speed & accuracy (RAG).   |  \n",
    "| **OpenAI Ada-002**      | 1536          | High accuracy (paid API).            |  \n",
    "| **BGE (BAAI)**          | 1024          | State-of-the-art open-source option. |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658a03c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
